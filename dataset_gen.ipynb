{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798c99b9",
   "metadata": {},
   "source": [
    "# Dataset Enrichment for LLM Fine-Tuning\n",
    "\n",
    "## Purpose\n",
    "This notebook focuses on **enhancing raw textual data** before fine-tuning a Large Language Model (LLM).\n",
    "\n",
    "\n",
    "\n",
    "This notebook explores **two dataset enrichment strategies**:\n",
    "1. POS-based linguistic enrichment\n",
    "2. LLM-based semantic enrichment\n",
    "\n",
    "Although POS tagging provides syntactic structure, **it does not capture semantic context**.\n",
    "In practice, fine-tuning on POS-tagged data led to:\n",
    "- Severe hallucinations\n",
    "- Loss of contextual grounding\n",
    "- Overfitting to syntactic patterns instead of meaning\n",
    "\n",
    "As a result:\n",
    "‚úÖ POS-based enrichment is used **only for analysis and experimentation**  \n",
    "‚ùå POS-enriched data is **NOT used** in the final fine-tuning dataset  \n",
    "\n",
    "Only **LLM-generated, context-rich samples** are saved and used for training.\n",
    "\n",
    "\n",
    "The enriched dataset is later used to fine-tune an LLM for high-quality article generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4a4cc",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n",
    "\n",
    "In this section, we import all required libraries for:\n",
    "- Text preprocessing\n",
    "- Linguistic analysis (POS tagging)\n",
    "- Local LLM inference\n",
    "- Dataset manipulation and storage\n",
    "\n",
    "These tools form the foundation for both enrichment pipelines used later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfc3a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Using cached weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in ./rag-env/lib/python3.11/site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./rag-env/lib/python3.11/site-packages (from spacy) (4.67.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./rag-env/lib/python3.11/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./rag-env/lib/python3.11/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./rag-env/lib/python3.11/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in ./rag-env/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./rag-env/lib/python3.11/site-packages (from spacy) (80.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./rag-env/lib/python3.11/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./rag-env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./rag-env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./rag-env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./rag-env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./rag-env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./rag-env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rag-env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./rag-env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in ./rag-env/lib/python3.11/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./rag-env/lib/python3.11/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp311-cp311-macosx_11_0_arm64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading murmurhash-1.0.15-cp311-cp311-macosx_11_0_arm64.whl (27 kB)\n",
      "Downloading preshed-3.0.12-cp311-cp311-macosx_11_0_arm64.whl (124 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp311-cp311-macosx_11_0_arm64.whl (653 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m653.1/653.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.10-cp311-cp311-macosx_11_0_arm64.whl (770 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m770.6/770.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Installing collected packages: wrapt, wasabi, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, confection, weasel, thinc, spacy\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/16\u001b[0m [spacy]m15/16\u001b[0m [spacy]open]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff89930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./rag-env/lib/python3.11/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./rag-env/lib/python3.11/site-packages (from datasets) (2.4.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./rag-env/lib/python3.11/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./rag-env/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./rag-env/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./rag-env/lib/python3.11/site-packages (from datasets) (4.67.2)\n",
      "Requirement already satisfied: xxhash in ./rag-env/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./rag-env/lib/python3.11/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: packaging in ./rag-env/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./rag-env/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./rag-env/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./rag-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./rag-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./rag-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./rag-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./rag-env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./rag-env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./rag-env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./rag-env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./rag-env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./rag-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./rag-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rag-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./rag-env/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./rag-env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./rag-env/lib/python3.11/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading pyarrow-23.0.0-cp311-cp311-macosx_12_0_arm64.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, fsspec, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/5\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: fsspec 2026.1.0 \u001b[32m0/5\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2026.1.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/5\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2026.1.00m \u001b[32m0/5\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [datasets]4/5\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.5.0 dill-0.4.0 fsspec-2025.10.0 multiprocess-0.70.18 pyarrow-23.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9345e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanparab/Desktop/Practice/node-course/Untitled Folder/tone/rag-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Spacy not found. Run: python -m spacy download en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "# Load spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Spacy loaded successfully\")\n",
    "except:\n",
    "    print(\"‚ùå Spacy not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd37f8",
   "metadata": {},
   "source": [
    "## Experimental: POS-Based Linguistic Analysis \n",
    "\n",
    "This section investigates whether POS-based enrichment can improve fine-tuning quality.\n",
    "\n",
    "POS tagging adds grammatical structure but **does not inject missing semantic context**.\n",
    "We evaluate this approach to understand its limitations before moving to LLM-based enrichment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa7a39",
   "metadata": {},
   "source": [
    "## Step 1: Factual Skeleton Extraction Using POS Tagging\n",
    "\n",
    "Rather than blindly expanding text, we first extract a **factual skeleton** from each article.\n",
    "\n",
    "The goal is to isolate:\n",
    "- Named entities (people, locations, organizations)\n",
    "- Numerical facts and dates\n",
    "- Core subject‚Äìverb‚Äìobject relationships\n",
    "\n",
    "This ensures that downstream LLM enrichment:\n",
    "- Preserves factual correctness\n",
    "- Avoids hallucinations\n",
    "- Maintains grounding in the source material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b03ef",
   "metadata": {},
   "source": [
    "### Factual Skeleton Extraction Function\n",
    "\n",
    "This function processes an article using spaCy and extracts:\n",
    "- Named entities (NER)\n",
    "- Numeric values\n",
    "- Key noun‚Äìverb‚Äìobject relationships\n",
    "\n",
    "Only **verifiable information** is retained, discarding stylistic or narrative fluff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_factual_skeleton(article):\n",
    "    \"\"\"Extract ONLY verifiable facts using POS tagging\"\"\"\n",
    "    \n",
    "    doc = nlp(article)\n",
    "    \n",
    "    facts = {\n",
    "        'entities': [],\n",
    "        'numbers': [],\n",
    "        'actions': [],\n",
    "        'quotes': [],\n",
    "    }\n",
    "    \n",
    "    # Extract Named Entities (People, Places, Organizations, Dates, Money)\n",
    "    seen_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in seen_entities:\n",
    "            facts['entities'].append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "            })\n",
    "            seen_entities.add(ent.text)\n",
    "    \n",
    "    # Extract Numbers with Context\n",
    "    for token in doc:\n",
    "        if token.like_num or token.pos_ == \"NUM\":\n",
    "            start = max(0, token.i - 3)\n",
    "            end = min(len(doc), token.i + 4)\n",
    "            context = doc[start:end].text\n",
    "            facts['numbers'].append({\n",
    "                'value': token.text,\n",
    "                'context': context\n",
    "            })\n",
    "    \n",
    "    # Extract Main Actions (Subject-Verb-Object)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.dep_ == \"ROOT\":\n",
    "            subject = None\n",
    "            obj = None\n",
    "            \n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
    "                    subject = child.text\n",
    "                elif child.dep_ in [\"dobj\", \"attr\", \"prep\"]:\n",
    "                    obj = child.text\n",
    "            \n",
    "            if subject:  # Only keep if we have a subject\n",
    "                facts['actions'].append({\n",
    "                    'subject': subject,\n",
    "                    'verb': token.lemma_,\n",
    "                    'object': obj\n",
    "                })\n",
    "    \n",
    "    # Extract Direct Quotes (EXACT preservation)\n",
    "    quote_pattern = r'\"([^\"]+)\"'\n",
    "    quotes = re.findall(quote_pattern, article)\n",
    "    facts['quotes'] = [q for q in quotes if len(q)>5]  # Skip very short\n",
    "    \n",
    "    return facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59427f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_facts_as_input(facts):\n",
    "    \"\"\"Convert facts into training input\"\"\"\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    # Group entities by type\n",
    "    entities_by_type = defaultdict(list)\n",
    "    for ent in facts['entities']:\n",
    "        entities_by_type[ent['label']].append(ent['text'])\n",
    "    \n",
    "    # Format entity types\n",
    "    label_names = {\n",
    "        'PERSON': 'People',\n",
    "        'ORG': 'Organizations',\n",
    "        'GPE': 'Locations',\n",
    "        'DATE': 'Dates',\n",
    "        'MONEY': 'Money/Amounts',\n",
    "        'CARDINAL': 'Numbers',\n",
    "        'EVENT': 'Events',\n",
    "    }\n",
    "    \n",
    "    for label, name in label_names.items():\n",
    "        if label in entities_by_type:\n",
    "            items = list(set(entities_by_type[label]))[:10]  # Max 10 per type\n",
    "            if items:\n",
    "                lines.append(f\"{name}: {', '.join(items)}\")\n",
    "    \n",
    "    # Add key actions\n",
    "    if facts['actions']:\n",
    "        actions_str = []\n",
    "        for action in facts['actions'][:5]:  # Top 5 actions\n",
    "            if action['object']:\n",
    "                actions_str.append(f\"{action['subject']} {action['verb']} {action['object']}\")\n",
    "            else:\n",
    "                actions_str.append(f\"{action['subject']} {action['verb']}\")\n",
    "        if actions_str:\n",
    "            lines.append(f\"Key events: {'; '.join(actions_str)}\")\n",
    "    \n",
    "    # Add quotes (PRESERVE EXACTLY!)\n",
    "    if facts['quotes']:\n",
    "        for i, quote in enumerate(facts['quotes'][:3], 1):\n",
    "            lines.append(f'Quote {i}: \"{quote}\"')\n",
    "    \n",
    "    # Add numbers with context\n",
    "    if facts['numbers']:\n",
    "        seen = set()\n",
    "        num_strs = []\n",
    "        for num in facts['numbers'][:8]:  # Max 8 numbers\n",
    "            if num['value'] not in seen:\n",
    "                num_strs.append(f\"{num['value']}\")\n",
    "                seen.add(num['value'])\n",
    "        if num_strs:\n",
    "            lines.append(f\"Numbers mentioned: {', '.join(num_strs)}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43e410",
   "metadata": {},
   "source": [
    "### Applying Factual Extraction Across the Dataset\n",
    "\n",
    "Each article in the dataset is processed to generate a compact factual representation.\n",
    "\n",
    "This representation acts as:\n",
    "- A high-signal conditioning input\n",
    "- A factual anchor for later semantic expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_pos_dataset(num_examples=10000):\n",
    "    \"\"\"Generate complete training dataset\"\"\"\n",
    "    \n",
    "    print(\"üìö Loading CNN/DailyMail dataset...\")\n",
    "    cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "    \n",
    "    print(f\"üîç Processing {num_examples} articles with POS tagging...\")\n",
    "    \n",
    "    training_pairs = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for i in tqdm(range(min(num_examples, len(cnn_dataset)))):\n",
    "        article = cnn_dataset[i]['article']\n",
    "        \n",
    "        # Skip very short articles\n",
    "        # if len(article) < 200:\n",
    "        #     skipped += 1\n",
    "        #     continue\n",
    "        \n",
    "        # Extract facts\n",
    "        \n",
    "        facts = extract_factual_skeleton(article)\n",
    "        \n",
    "        \n",
    "        # Format as input\n",
    "        facts_input = format_facts_as_input(facts)\n",
    "        \n",
    "        # # Skip if too few facts\n",
    "        # if len(facts_input) < 50 or not facts['entities']:\n",
    "        #     skipped += 1\n",
    "        #     continue\n",
    "        \n",
    "        # Create training pair\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a professional journalist. Write clear, factual news articles using ONLY the information provided. Do not invent names, quotes, numbers, or any other details. Use exact quotes as given.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Write a professional news article using these facts:\\n\\n{facts_input}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": article\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        training_pairs.append({\n",
    "            \"messages\": messages,\n",
    "            \"facts_input\": facts_input,\n",
    "            \"article\": article,\n",
    "            \"extracted_facts\": facts\n",
    "        })\n",
    "\n",
    "        #print( \"facts_input:\", facts_input,\"extracted_facts: \", facts)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(training_pairs)} training pairs\")\n",
    "    print(f\"‚ö†Ô∏è  Skipped {skipped} articles (too short or insufficient facts)\")\n",
    "    \n",
    "    return Dataset.from_list(training_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a99761",
   "metadata": {},
   "source": [
    "### Why POS Tagging Falls Short for Fine-Tuning\n",
    "\n",
    "While POS tagging helps models learn syntax, it fails to:\n",
    "- Capture implicit reasoning\n",
    "- Preserve narrative flow\n",
    "- Encode domain-specific context\n",
    "\n",
    "When used for fine-tuning, POS-enriched inputs caused the model to:\n",
    "- Generate syntactically valid but semantically incorrect text\n",
    "- Hallucinate facts due to missing contextual grounding\n",
    "\n",
    "Therefore, POS tagging is treated as an **exploratory preprocessing step**, not a training signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706ee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "POS-BASED FACT-CONSTRAINED DATASET GENERATOR\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting dataset generation...\n",
      "This will take ~1 hour for 10,000 examples\n",
      "(Much faster than LLM-based approach!)\n",
      "\n",
      "üìö Loading CNN/DailyMail dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 10000 articles with POS tagging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [17:02<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created 10000 training pairs\n",
      "‚ö†Ô∏è  Skipped 0 articles (too short or insufficient facts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 74420.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Dataset saved to: pos_constrained_cnn_dataset\n",
      "\n",
      "======================================================================\n",
      "üìã EXAMPLE 1:\n",
      "======================================================================\n",
      "\n",
      "üîµ INPUT (Extracted Facts):\n",
      "People: the Order of the Phoenix, Peter Shaffer's, Potter, Harry Potter, Radcliffe, Londoner, Daniel Radcliffe, Rudyard Kipling\n",
      "Organizations: Reuters\n",
      "Locations: England, UK, LONDON\n",
      "Dates: Earlier this year, Monday, 2007, later this year, earlier this month, last month\n",
      "Money/Amounts: $41.1 million, ¬£20 million\n",
      "Numbers: two, about four, six, 18, five, one\n",
      "Key events: LONDON gain access; actor say To; he tell interviewer; I think; agent have comment\n",
      "Quote 1: \"Harry Potter and the Order of the Phoenix\"\n",
      "Quote 2: \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"\n",
      "Quote 3: \"I don't think I'll be particularly extravagant. \"\n",
      "Numbers mentioned: 20, million, 41.1, 18, one, 10\n",
      "\n",
      "üü¢ OUTPUT (Article):\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported ¬£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his ca...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìã EXAMPLE 2:\n",
      "======================================================================\n",
      "\n",
      "üîµ INPUT (Extracted Facts):\n",
      "People: Peter Grossman, Grossman Burn Center, Wayne Drash, Barbara Friedman, Arwa Damon, Youssif, Zainab\n",
      "Organizations: CNN, the Ministry of Health, the Children's Burn Foundation\n",
      "Locations: Sherman Oaks, the United States, California, Baghdad, Iraq, Atlanta, BAGHDAD, America\n",
      "Dates: Wednesday, Friday, January day, 6 next Friday, 5-year-old, between six months to a year, this day, January 15\n",
      "Numbers: the tens of thousands, millions\n",
      "Key events: Youssif hold hand; parents talk about; Youssif hold hand; He wear mask; Youssif turn 6\n",
      "Quote 1: \"I was so happy I didn't know what to do with myself,\"\n",
      "Quote 2: \"I didn't think the reaction would be this big.\"\n",
      "Quote 3: \"We just want to thank everyone who has come forward,\"\n",
      "Numbers mentioned: 5, 6, 15, one, six, first\n",
      "\n",
      "üü¢ OUTPUT (Article):\n",
      "BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. Youssif holds his sister's hand Friday. He's wearing a facial mask of...\n",
      "======================================================================\n",
      "\n",
      "üìä DATASET STATISTICS:\n",
      "  Total examples: 10000\n",
      "  Avg facts length: 803 chars\n",
      "  Avg article length: 3695 chars\n",
      "  Expansion ratio: 4.6x\n",
      "\n",
      "‚úÖ Dataset ready for training!\n",
      "\n",
      "Next steps:\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"POS-BASED FACT-CONSTRAINED DATASET GENERATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate dataset\n",
    "    print(\"\\nüöÄ Starting dataset generation...\")\n",
    "    print(\"This will take ~1 hour for 10,000 examples\")\n",
    "    print(\"(Much faster than LLM-based approach!)\\n\")\n",
    "    dataset = create_pos_dataset(num_examples=10000)\n",
    "    \n",
    "    \n",
    "    # Save\n",
    "    output_path = \"pos_constrained_cnn_dataset\"\n",
    "    dataset.save_to_disk(output_path)\n",
    "    print(f\"\\nüíæ Dataset saved to: {output_path}\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã EXAMPLE 1:\")\n",
    "    print(\"=\"*70)\n",
    "    example = dataset[0]\n",
    "    print(\"\\nüîµ INPUT (Extracted Facts):\")\n",
    "    print(example['facts_input'])\n",
    "    print(\"\\nüü¢ OUTPUT (Article):\")\n",
    "    print(example['article'][:400] + \"...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã EXAMPLE 2:\")\n",
    "    print(\"=\"*70)\n",
    "    example = dataset[5]\n",
    "    print(\"\\nüîµ INPUT (Extracted Facts):\")\n",
    "    print(example['facts_input'])\n",
    "    print(\"\\nüü¢ OUTPUT (Article):\")\n",
    "    print(example['article'][:400] + \"...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nüìä DATASET STATISTICS:\")\n",
    "    print(f\"  Total examples: {len(dataset)}\")\n",
    "    \n",
    "    avg_facts_length = sum(len(ex['facts_input']) for ex in dataset) / len(dataset)\n",
    "    avg_article_length = sum(len(ex['article']) for ex in dataset) / len(dataset)\n",
    "    \n",
    "    print(f\"  Avg facts length: {avg_facts_length:.0f} chars\")\n",
    "    print(f\"  Avg article length: {avg_article_length:.0f} chars\")\n",
    "    print(f\"  Expansion ratio: {avg_article_length/avg_facts_length:.1f}x\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset ready for training!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "  \n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9eaf583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Loading POS-constrained dataset...\n",
      "‚úÖ Loaded 10000 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìö Loading POS-constrained dataset...\")\n",
    "\n",
    "try:\n",
    "    dataset = Dataset.load_from_disk(\"pos_constrained_cnn_dataset\")\n",
    "    print(f\"‚úÖ Loaded {len(dataset)} examples\")\n",
    "except:\n",
    "    print(\"‚ùå Dataset not found!\")\n",
    "    print(\"Run: python generate_pos_dataset.py first\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08b2f0",
   "metadata": {},
   "source": [
    "### Observations from POS-Based Enrichment\n",
    "\n",
    "Key observations:\n",
    "- POS tagging preserves grammar but strips meaning\n",
    "- Context loss outweighs syntactic benefits\n",
    "- Fine-tuned models trained on this data hallucinate heavily\n",
    "\n",
    "This confirms that **semantic richness is more important than grammatical annotation** for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347624",
   "metadata": {},
   "source": [
    "POS tagged dataset looked like this, but wasn't able to provide desired output.\n",
    "\n",
    "\n",
    "The reason being that the LLM model gets trained on only the data rather than how it is being used in the sentence. (Final Loss :1.9 over 3000 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df38f1b",
   "metadata": {},
   "source": [
    "## Using OLLAMA to generate rough notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796b018",
   "metadata": {},
   "source": [
    "## Moving Beyond POS: Semantic Enrichment with LLMs\n",
    "\n",
    "Given the limitations of POS-based enrichment, we shift focus to **LLM-driven dataset enhancement**.\n",
    "\n",
    "LLMs can:\n",
    "- Infer missing context\n",
    "- Preserve factual grounding\n",
    "- Generate coherent explanatory text\n",
    "\n",
    "This makes LLM-enriched data far more suitable for fine-tuning.\n",
    "\n",
    "\n",
    "Here we will using OLLAMA generate rough notes (journalist style) for a given CNN article. Reverse engineer the article so now we have a proper data to train on. That is, given rough notes this is how final article looks. \n",
    "\n",
    "This helps us to very efficiently capture the required writing style for a article writer for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca58c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama module found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(\"‚úÖ Ollama module found\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Ollama not installed!\")\n",
    "    print(\"\\nInstall:\")\n",
    "    print(\"  pip install ollama\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef42ab",
   "metadata": {},
   "source": [
    "## Final Dataset Pipeline: LLM-Based Semantic Enrichment\n",
    "\n",
    "This section generates the **final dataset used for fine-tuning**.\n",
    "\n",
    "All samples produced here:\n",
    "- Retain original factual content\n",
    "- Add explanatory context\n",
    "- Are saved and exported for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1280d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rough_notes(article, style=\"bullet\"):\n",
    "    \"\"\"\n",
    "    Use local LLM to convert polished article into rough notes\n",
    "    \n",
    "    Styles:\n",
    "    - \"bullet\": Bullet point format\n",
    "    - \"brief\": Very short sentences\n",
    "    - \"fragments\": Sentence fragments\n",
    "    - \"minimal\": Absolute minimum info\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"bullet\": \"\"\"Convert this polished article into rough bullet-point notes that a journalist might write:\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "Requirements:\n",
    "- Bullet points only\n",
    "- Keep all key facts (names, numbers, dates, quotes)\n",
    "- Remove fancy language\n",
    "- Remove transitions and context\n",
    "- Keep it brief\n",
    "\n",
    "Rough notes:\"\"\",\n",
    "\n",
    "        \"brief\": \"\"\"Convert this into very brief rough notes with short sentences:\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "Requirements:\n",
    "- Very short, simple sentences\n",
    "- Keep facts but remove fluff\n",
    "- No fancy words\n",
    "- Sound like quick notes\n",
    "\n",
    "Rough notes:\"\"\",\n",
    "\n",
    "        \"fragments\": \"\"\"Convert this into rough note fragments (incomplete sentences):\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "Requirements:\n",
    "- Sentence fragments OK\n",
    "- Keep key facts only\n",
    "- Remove adjectives\n",
    "- Very rough style\n",
    "\n",
    "Notes:\"\"\",\n",
    "\n",
    "        \"minimal\": \"\"\"Extract ONLY the core facts from this article in minimal note form:\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "What happened? Who? When? How much? Quote?\n",
    "\n",
    "Brief facts:\"\"\"\n",
    "    }\n",
    "    \n",
    "    prompt = prompts.get(style, prompts[\"bullet\"]).format(article=article[:1500])\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3.2:3b',  # Fast, good quality\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                'temperature': 0.3,  # Low = more consistent\n",
    "                'top_p': 0.9,\n",
    "                'num_predict': 200,  # Short responses\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        rough_notes = response['response'].strip()\n",
    "        \n",
    "        # Clean up\n",
    "        rough_notes = rough_notes.replace('**', '')  # Remove markdown bold\n",
    "        rough_notes = rough_notes.strip()\n",
    "        \n",
    "        return rough_notes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a0aa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.2:3b', modified_at=datetime.datetime(2026, 1, 14, 0, 27, 52, 40183, tzinfo=TzInfo(-28800)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='gemma2:2b', modified_at=datetime.datetime(2025, 7, 11, 11, 17, 27, 384119, tzinfo=TzInfo(-25200)), digest='8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7', size=1629518495, details=ModelDetails(parent_model='', format='gguf', family='gemma2', families=['gemma2'], parameter_size='2.6B', quantization_level='Q4_0')), Model(model='llama3:latest', modified_at=datetime.datetime(2025, 7, 11, 11, 15, 21, 694869, tzinfo=TzInfo(-25200)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 5, 3, 16, 7, 44, 154770, tzinfo=TzInfo(-25200)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307faef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since cnn_dailymail couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '3.0.0' at /Users/aryanparab/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d (last modified on Tue Jan 13 00:41:11 2026).\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541/541 [1:22:55<00:00,  9.20s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "training_pairs = []\n",
    "skipped = 0\n",
    "start_time = time.time()\n",
    "completed =4459\n",
    "num_examples = 5000\n",
    "style=\"bullet\"\n",
    "with open(\"data.jsonl\", \"a\") as f:\n",
    "\n",
    "    for i in tqdm( range(completed, min(num_examples, len(cnn_dataset)))):\n",
    "            article = cnn_dataset[i]['article']\n",
    "            \n",
    "            # Skip very short articles\n",
    "            if len(article) < 300:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Generate rough notes using LLM\n",
    "            rough_notes = generate_rough_notes(article, style=style)\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a professional journalist. Expand rough notes into complete, well-written news articles. Maintain all facts while adding proper structure and professional language.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Expand these rough notes into a professional news article:\\n\\n{rough_notes}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": article\n",
    "                }\n",
    "            ]\n",
    "            entry = {\n",
    "                \"messages\": messages,\n",
    "                \"rough_notes\": rough_notes,\n",
    "                \"polished_article\": article,\n",
    "                \"style\": style\n",
    "            }\n",
    "            training_pairs.append(entry)\n",
    "            \n",
    "            json.dump(entry, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "new_df  = Dataset.from_list(training_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to my_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = training_pairs\n",
    "\n",
    "# Specify the filename\n",
    "filename = \"my_data.json\"\n",
    "\n",
    "# Open the file in write mode ('w') and use json.dump()\n",
    "try:\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Successfully wrote data to {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error writing to file {filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23851d53",
   "metadata": {},
   "source": [
    "## Parsing the data to correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fix_json_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Fixes JSON file with multiple objects (JSONL format) to proper JSON array\n",
    "\n",
    "    Converts:\n",
    "    {\"key\": \"value1\"}\n",
    "    {\"key\": \"value2\"}\n",
    "\n",
    "    To:\n",
    "    [\n",
    "      {\"key\": \"value1\"},\n",
    "      {\"key\": \"value2\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üîß Fixing {input_file}...\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                data.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è Error on line {line_num}: {e}\")\n",
    "                print(f\"   Line content: {line[:100]}...\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully parsed {len(data)} objects\")\n",
    "\n",
    "    # Save as proper JSON array\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"üíæ Saved fixed JSON to: {output_file}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"  Total records: {len(data)}\")\n",
    "\n",
    "    if data:\n",
    "        print(f\"\\nüìã First record preview:\")\n",
    "        first = data[0]\n",
    "        print(f\"  Keys: {list(first.keys())}\")\n",
    "        if 'rough_notes' in first:\n",
    "            print(f\"  Rough notes length: {len(first['rough_notes'])} chars\")\n",
    "        if 'polished_article' in first:\n",
    "            print(f\"  Article length: {len(first['polished_article'])} chars\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXAMPLE_FORMAT = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a professional journalist. Expand rough notes into complete, well-written news articles. Maintain all facts while adding proper structure and professional language.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Expand these rough notes into a professional news article:\\n\\n‚Ä¢ Bullet point 1\\n‚Ä¢ Bullet point 2\\n‚Ä¢ etc...\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Full polished article text here...\"\n",
    "            }\n",
    "        ],\n",
    "        \"rough_notes\": \"‚Ä¢ Bullet point 1\\n‚Ä¢ Bullet point 2\\n‚Ä¢ etc...\",\n",
    "        \"polished_article\": \"Full polished article text here...\"\n",
    "    }\n",
    "   \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def validate_dataset(json_path):\n",
    "    \"\"\"Validate that your dataset is in the correct format\"\"\"\n",
    "\n",
    "    print(\"üîç Validating dataset...\")\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File '{json_path}' not found!\")\n",
    "        return False\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Error: Invalid JSON format - {e}\")\n",
    "        return False\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"‚ùå Error: JSON must be a list of objects\")\n",
    "        return False\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"‚ùå Error: Dataset is empty!\")\n",
    "        return False\n",
    "\n",
    "    print(f\"‚úÖ Found {len(data)} examples\")\n",
    "\n",
    "    # Check first example\n",
    "    example = data[0]\n",
    "    required_keys = ['messages', 'rough_notes', 'polished_article']\n",
    "\n",
    "    for key in required_keys:\n",
    "        if key not in example:\n",
    "            print(f\"‚ùå Error: Missing key '{key}' in first example\")\n",
    "            return False\n",
    "\n",
    "    # Validate messages structure\n",
    "    if not isinstance(example['messages'], list) or len(example['messages']) != 3:\n",
    "        print(\"‚ùå Error: 'messages' must be a list with 3 items (system, user, assistant)\")\n",
    "        return False\n",
    "\n",
    "    roles = [msg.get('role') for msg in example['messages']]\n",
    "    if roles != ['system', 'user', 'assistant']:\n",
    "        print(f\"‚ùå Error: Message roles must be ['system', 'user', 'assistant'], got {roles}\")\n",
    "        return False\n",
    "\n",
    "    # Check for empty content\n",
    "    for i, msg in enumerate(example['messages']):\n",
    "        if not msg.get('content'):\n",
    "            print(f\"‚ùå Error: Empty content in message {i}\")\n",
    "            return False\n",
    "\n",
    "    print(\"‚úÖ Dataset format is valid!\")\n",
    "\n",
    "    # Statistics\n",
    "    print(\"\\nüìä DATASET STATISTICS:\")\n",
    "    print(f\"  Total examples: {len(data)}\")\n",
    "\n",
    "    avg_notes = sum(len(ex['rough_notes']) for ex in data) / len(data)\n",
    "    #avg_article = sum(len(ex['polished_article']) for ex in data) / len(data)\n",
    "\n",
    "    print(f\"  Average rough notes length: {avg_notes:.0f} chars\")\n",
    "    # print(f\"  Average article length: {avg_article:.0f} chars\")\n",
    "    # print(f\"  Expansion ratio: {avg_article/avg_notes:.1f}x\")\n",
    "\n",
    "    # Show first example\n",
    "    print(\"\\nüìã FIRST EXAMPLE:\")\n",
    "    print(\"\\nRough notes:\")\n",
    "    print(example['rough_notes'][:200] + \"...\" if len(example['rough_notes']) > 200 else example['rough_notes'])\n",
    "    print(\"\\nPolished article:\")\n",
    "    print(example['polished_article'][:300] + \"...\" if len(example['polished_article']) > 300 else example['polished_article'])\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_training_format(input_json_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Convert your raw data to the training format\n",
    "\n",
    "    Assumes input format like:\n",
    "    [\n",
    "        {\n",
    "            \"rough_notes\": \"...\",\n",
    "            \"polished_article\": \"...\",\n",
    "            \"style\": \"bullet\"  # optional\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üîÑ Converting {input_json_path} to training format...\")\n",
    "\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    for item in raw_data:\n",
    "        # Create the messages format\n",
    "        if 'polished_article' not in item:continue\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a professional journalist. Expand rough notes into complete, well-written news articles. Maintain all facts while adding proper structure and professional language.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Expand these rough notes into a professional news article:\\n\\n{item['rough_notes']}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": item['polished_article']\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        training_data.append({\n",
    "            \"messages\": messages,\n",
    "            \"rough_notes\": item['rough_notes'],\n",
    "            \"polished_article\": item['polished_article']\n",
    "        })\n",
    "\n",
    "    # Save\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Converted {len(training_data)} examples\")\n",
    "    print(f\"üíæ Saved to: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"/content/data.json\"  # ‚Üê Your broken file\n",
    "output_filename = \"/content/cnn_training_data_fixed.json\"  # ‚Üê Fixed output\n",
    "\n",
    "fixed_data = fix_json_file(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a32082",
   "metadata": {},
   "source": [
    "üîß Fixing /content/data.json...\n",
    "‚úÖ Successfully parsed 4999 objects\n",
    "üíæ Saved fixed JSON to: /content/cnn_training_data_fixed.json\n",
    "\n",
    "üìä Stats:\n",
    "  Total records: 4999\n",
    "\n",
    "üìã First record preview:\n",
    "  Keys: ['messages', 'rough_notes', 'polished_article', 'style']\n",
    "  Rough notes length: 571 chars\n",
    "  Article length: 2527 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbdb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validate_dataset(output_filename)\n",
    "convert_to_training_format(output_filename, \"/content/cnn_training_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57133c9d",
   "metadata": {},
   "source": [
    "üîç Validating dataset...\n",
    "‚úÖ Found 4999 examples\n",
    "‚úÖ Dataset format is valid!\n",
    "\n",
    "üìä DATASET STATISTICS:\n",
    "  Total examples: 4999\n",
    "  Average rough notes length: 585 chars\n",
    "\n",
    "üìã FIRST EXAMPLE:\n",
    "\n",
    "Rough notes:\n",
    "‚Ä¢ Daniel Radcliffe turns 18 on Monday with a ¬£20 million fortune\n",
    "‚Ä¢ He won't spend the money on luxury items like cars or parties\n",
    "‚Ä¢ \"I don't plan to be one of those people who, as soon as they turn 18,...\n",
    "\n",
    "Polished article:\n",
    "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported ¬£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappoi...\n",
    "üîÑ Converting /content/cnn_training_data_fixed.json to training format...\n",
    "‚úÖ Converted 4999 examples\n",
    "üíæ Saved to: /content/cnn_training_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2fc7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce66403",
   "metadata": {},
   "source": [
    "## Saving Only LLM-Enriched Data\n",
    "\n",
    "Although multiple enrichment strategies were explored, **only LLM-generated outputs are persisted**.\n",
    "\n",
    "This decision is based on empirical results showing:\n",
    "- Lower hallucination rates\n",
    "- Better contextual coherence\n",
    "- Superior article-writing capability\n",
    "\n",
    "POS-enriched representations are intentionally excluded from the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aab4f",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "In this notebook, we evaluated multiple dataset enrichment strategies:\n",
    "\n",
    "- POS-based enrichment was tested and rejected due to context loss and hallucination\n",
    "- LLM-based enrichment was selected for its semantic depth and factual grounding\n",
    "\n",
    "The final dataset:\n",
    "- Contains **only LLM-generated enriched text**\n",
    "- Is optimized for supervised fine-tuning\n",
    "- Produces significantly more coherent and reliable models\n",
    "\n",
    "This reinforces a key insight:\n",
    "**For LLM fine-tuning, semantic context matters far more than syntactic annotation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a4b24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
